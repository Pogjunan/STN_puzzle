{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c86849a-92e1-4cc2-9843-e62ba89c154f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           503Gi       111Gi       280Gi       1.1Gi       110Gi       387Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -mh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5e157-e861-44d2-8c5d-a6db298f21ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `STN(Spatial Transformer Networks) 공간 변형 네트워크`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326e437-79a1-4e99-b856-cc2607bccb7a",
   "metadata": {},
   "source": [
    "----\n",
    "[튜토리얼]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd6722-4721-48d6-8a8b-6e163e6abd31",
   "metadata": {},
   "source": [
    "<설명> <br>\n",
    "- Visual Attention 을 이용하여 데이터증강(augment)에서 이득을 본다.\n",
    "<br>\n",
    "- 미분 가능한 어텐션의 일반화이다.\n",
    "<br>\n",
    "- 이미지의 관심 영역을 자르고 ,크기 조정, 방향 수정이 가능하다.\n",
    "<br>\n",
    "- CNN 은 크기조정,회전 등에 민감하여 STN 이 극복에 유ㅜ용하다.\n",
    "<br>\n",
    "- 가장큰 장점으로 아주 작은 수정으로 기존의 CNN 과 간단하게 연결이 가능하다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063ca58-6edf-459c-8b4f-feda280fae52",
   "metadata": {
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235deb74-7a3a-429e-9db2-d678f4d0d84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f0cd2bbbbe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이센스: BSD\n",
    "# 저자: Ghassen Hamrouni\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.ion()   # 대화형 모드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4bfc4-fe40-402c-b0e4-8636ea832666",
   "metadata": {},
   "source": [
    "# 1. Loading Data (데이터불러오기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab893ee-9ab6-4ff4-ae61-c999ddd9be9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')] # Mozilla 제가 옛날에 html , css , java-script 의 기초를 공부했던 사이트!!\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 학습용 데이터셋\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
    "# 테스트용 데이터셋\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b163082-a74a-4d47-af60-eb7b3c319624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f0cd2bbae60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader #train_loader 는 우리가 이미 만들었던대로 데이터가 정제되어 딥러닝에 들어가기 깔끔하게 만들어진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6666f19a-a1e7-4eb1-a5c4-773e3e1e7b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_loader안의 실제값 한개 뽑아서 확인 \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.__next__()\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b11adb-b62c-45b0-ab41-ee78279490a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "현재\n",
    "train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True, num_workers=4\n",
    "를 보면\n",
    "train 데이터로 쓸 것이고 그냥 데이터 이미지를  tensor 로 바꾸고 정규화시키고 batch_size는 64로 만들겠다는 의미입니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9454fe-1784-45c4-aa19-a9b190049d42",
   "metadata": {},
   "source": [
    "# 2. STN 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a863312-7e13-48ea-94c9-715ada37d68b",
   "metadata": {},
   "source": [
    "`(1) localization network(위치결정네트워크) 구성` : 일반적인 CNN 의 역할. 공간 변환을 통해 파라미터를 예측하고 신경망이 자동 학습을 하여 데이터셋에서 명시되지는 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4276fc9-bcba-47d1-8500-aabcea459fd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "`(2) grid generator(그리드 생성기)` : 출력 이미지와 대응되는 각 픽설에 이미지 내 좌표 공간(grid)을 생성. \n",
    "`(3) sampler(샘플러)` : 공간 변환 파라미터를 입력 이미지에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583be4d-55c6-484f-aaf6-88d017a880c8",
   "metadata": {},
   "source": [
    "`-` affine_grid 및 grid_sample 모듈이 포함된 최신 버전의 PyTorch가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002ca4c-b59b-40d3-a2fb-bde64d091947",
   "metadata": {
    "tags": []
   },
   "source": [
    "`(3) 함수보기전 데이터 미리보기` : <br>\n",
    "    `-` MNIST 데이터이므로 흑백사진이라서 1*28*28 의 이미지라는 것을 알 수 있다. <br>\n",
    "    `-` train_loader 을 통해 transform 을 지나가며 batch_size = 64 이고 (평균: 0.13 , 분산 : 0.3 으로 정규화진행) (batch_size, 1, 28, 28) 인 tensor 가 나오게됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf797ca-18ec-405a-a662-8cded67d5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) #채널: 입력 1 , 출력10 , 5*5 필터사용\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # 공간 변환을 위한 위치 결정 네트워크 (localization-network)\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        ) # tensor([64,10,3,3]) 그래서 밑에보면 10*3*3 을 넣는다.\n",
    "\n",
    "        # [3 * 2] 크기의 아핀(affine) 행렬에 대해 예측\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # 항등 변환(identity transformation)으로 가중치/바이어스 초기화\n",
    "        self.fc_loc[2].weight.data.zero_() #3번째 layer 가중치 0 만들기\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # STN의 forward 함수\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3) #미니배치크기에 맞춘다는 의미\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력을 변환\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # 일반적인 forward pass를 수행\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x)) #50개로 변환\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x) #10개로 변환 \n",
    "        return F.log_softmax(x, dim=1) #10개를 log softmax 실시\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68ffc9-09e6-4712-827f-50189ccdd1bd",
   "metadata": {},
   "source": [
    "[해석] <br>\n",
    "`-` 첫 번째 컨볼루션 레이어 (nn.Conv2d(1, 8, kernel_size=7)):\n",
    "<br>\n",
    "- 입력 채널: 1 (흑백 이미지이므로)\n",
    "- 출력 채널: 8\n",
    "- 커널 크기: 7x7\n",
    "- 출력 크기: (28 - 7 + 2 * 0) + 1 = 22\n",
    "> 따라서 출력 텐서 크기는 (64, 8, 22, 22)가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716f7bd-8374-483c-b5d0-bee4e29273ec",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39671e-a978-49b5-9ad0-2ea35597801a",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 첫 번째 맥스 풀링 레이어 (nn.MaxPool2d(2, stride=2)):\n",
    "\n",
    "- 풀링 윈도우 크기: 2x2\n",
    "- 스트라이드: 2\n",
    "- 출력 크기: (22 - 2) / 2 + 1 = 11\n",
    "> 따라서 출력 텐서 크기는 (64, 8, 11, 11)이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b96ad2-21bc-407b-b952-6c6a8702ec86",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7ce1b-65ac-4dc3-8065-acc09cd0b28b",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` **ReLU** 활성화 함수 (nn.ReLU(True)):\n",
    "<br>\n",
    "활성화 함수를 통과하므로 텐서 크기는 변하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20039753-047e-4d5f-9d3a-65e91c36289e",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 두 번째 컨볼루션 레이어 (nn.Conv2d(8, 10, kernel_size=5)):\n",
    "<br>\n",
    "- 입력 채널: 8\n",
    "- 출력 채널: 10\n",
    "- 커널 크기: 5x5\n",
    "- 출력 크기: (11 - 5 + 2 * 0) + 1 = 7\n",
    "> 따라서 출력 텐서 크기는 (64, 10, 7, 7)이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded25cd-85ab-4080-b527-3f24ae8ce000",
   "metadata": {},
   "source": [
    "`-` 두 번째 맥스 풀링 레이어 (nn.MaxPool2d(2, stride=2)):\n",
    "<br>\n",
    "- 풀링 윈도우 크기: 2x2\n",
    "- 스트라이드: 2\n",
    "- 출력 크기: (7 - 2) / 2 + 1 = 3\n",
    "> 따라서 출력 텐서 크기는 (64, 10, 3, 3)이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080c16-a6b5-45d3-b54d-bfd954d6f937",
   "metadata": {},
   "source": [
    "> 따라서 `self.localization`에 의해 처리된 후의 텐서 크기는 (64, 10, 3, 3)이 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65455a6d-b847-4d8a-bddb-1665a2d8d2d0",
   "metadata": {},
   "source": [
    "```python\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3) #미니배치크기에 맞춘다는 의미\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "```\n",
    "<br>\n",
    "1.\n",
    "xs = self.localization(x): Localization Network를 통과한 결과인 xs는 크기가 (64, 10, 3, 3)인 텐서입니다.\n",
    "<br>\n",
    "2.\n",
    "xs = xs.view(-1, 10 * 3 * 3): xs를 2D 텐서로 평탄화합니다. 결과적으로 xs는 크기가 (64, 90)인 텐서로 변합니다.\n",
    "<br>\n",
    "3.\n",
    "theta = self.fc_loc(xs): 평탄화된 xs를 Fully Connected 레이어에 통과시켜 공간 변환 매개변수 theta를 얻습니다. theta의 크기는 (64, 6)입니다. #fc_loc 라는 함수를 linear(90,32) -> linear(32,6) 을 진행했었다.\n",
    "<br>\n",
    "4.\n",
    "theta = theta.view(-1, 2, 3): theta를 크기가 (64, 2, 3)인 3D 텐서로 변환합니다. 이는 공간 변환 매개변수를 나타냅니다.\n",
    "<br>\n",
    "5.\n",
    "grid = F.affine_grid(theta, x.size()): Affine 그리드를 계산합니다. 이 그리드는 입력 이미지에 적용할 공간적인 변환을 정의합니다.\n",
    "<br>\n",
    "`-` affine_grid : x.size() 의 크기로 이미지를 공간적 변환을 선현 변환과 평행 이동으로 만드는 함수입니다.\n",
    "<br>\n",
    "6.\n",
    "x = F.grid_sample(x, grid): Affine 그리드를 사용하여 입력 이미지 x에 공간적인 변환을 적용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b6d24-c76f-4451-9f7f-9d9bd39a9ee6",
   "metadata": {},
   "source": [
    "## `최종적인 x의 크기는 (64, 1, 28, 28)로 유지됩니다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178204e8-defd-4304-8ea0-23e91f502b2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# forward(x)\n",
    "\n",
    "> 이제 forward 로 input, output 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb94572-11ff-4369-8e7c-635f9f202b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` return F.log_softmax(x, dim=1) : x 가 결국10개의 output 층으로 나타나고 이 후 log softmax 에 의해 10가지를 dim=1 인 각 클래스에 속할 확률의 로그값의 텐서 10개로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5adf362-4504-4383-affd-d2e4b75060a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "#\n",
    "# MNIST 데이터셋에서 STN의 성능을 측정하기 위한 간단한 테스트 절차\n",
    "#\n",
    "\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval() #pytorch 에서 모델 평가하는 메서드이다.\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 손실 합하기\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # 로그-확률의 최대값에 해당하는 인덱스 가져오기\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(test_loss, correct, len(test_loader.dataset),\n",
    "                      100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8da29-a16a-4ac0-8eb6-e52b73cf14f2",
   "metadata": {},
   "source": [
    "# STN 결과 시각화하기\n",
    "`-` 매커니즘의 결과를 보고 시각화에 도움되는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b999000-cc7d-459c-9b5c-3d792b850d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_image_np(inp):\n",
    "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "# 학습 후 공간 변환 계층의 출력을 시각화하고, 입력 이미지 배치 데이터 및\n",
    "# STN을 사용해 변환된 배치 데이터를 시각화 합니다.\n",
    "\n",
    "\n",
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        # 학습 데이터의 배치 가져오기\n",
    "        data = next(iter(test_loader))[0].to(device)\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_input_tensor = model.stn(data).cpu()\n",
    "\n",
    "        in_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(input_tensor))\n",
    "\n",
    "        out_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(transformed_input_tensor))\n",
    "\n",
    "        # 결과를 나란히 표시하기\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(in_grid)\n",
    "        axarr[0].set_title('Dataset Images')\n",
    "\n",
    "        axarr[1].imshow(out_grid)\n",
    "        axarr[1].set_title('Transformed Images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed166e8-b970-4bd9-95b1-49eb4a95ca99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 20 + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "# 일부 입력 배치 데이터에서 STN 변환 결과를 시각화\n",
    "visualize_stn()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0163ca-e806-4a8b-9e5e-7e282657a4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf)",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
